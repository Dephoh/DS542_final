{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee06565-5d78-4c9f-9ca0-df11391a70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale import process_to_size\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af74ceb5-d857-4fa2-8542-568d3bfc5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./selected/nm0705356_rm860471040_1989-7-23_2007.jpg\n",
      "./selected/nm0002122_rm3114050048_1951-10-30_2011.jpg\n",
      "./selected/nm0000553_rm2186975488_1952-6-7_2002.jpg\n",
      "./selected/nm0004812_rm1021102336_1951-7-24_1975.jpg\n",
      "./selected/nm0911320_rm2216791808_1964-11-14_2007.jpg\n",
      "./selected/nm0001719_rm93310464_1934-2-13_1972.jpg\n",
      "./selected/nm2832695_rm830128640_1985-2-7_2008.jpg\n",
      "./selected/nm0000330_rm21662976_1944-2-13_2010.jpg\n",
      "./selected/nm0917060_rm1072422144_1978-9-3_2011.jpg\n",
      "./selected/nm0035488_rm2776335616_1966-12-4_2011.jpg\n"
     ]
    }
   ],
   "source": [
    "def get_file_paths(directory):\n",
    "   \"\"\"goes through the given directory and returns a list of the paths to all the files\"\"\"\n",
    "   file_paths = []\n",
    "   \n",
    "   # walk through directory and subdirectories\n",
    "   for root, dirs, files in os.walk(directory):\n",
    "       for file in files:\n",
    "           # create full file path\n",
    "           file_path = os.path.join(root, file)\n",
    "           file_paths.append(file_path)\n",
    "           \n",
    "   return file_paths\n",
    "\n",
    "directory = \"./selected/\"\n",
    "paths = get_file_paths(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1672a11c-6764-4c8a-8915-792e8f1b6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102750/102750 [14:43<00:00, 116.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# This is the scaling part\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    # print(path)\n",
    "    img = process_to_size(path,target_size=224)\n",
    "    img.save(f\"./scaled/{path[11:]}\") # the slicing is to remove the \"selected\" directory from the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cda38c3-e3b0-4612-aef8-a0bf213c747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102750it [00:59, 1732.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#This script is for visualizing how the scaling affected images\n",
    "\n",
    "new_paths = get_file_paths(\"./scaled/\")\n",
    "for i in tqdm(zip(new_paths,paths)):\n",
    "    scaled = Image.open(i[0])\n",
    "    orig = Image.open(i[1])\n",
    "    if (scaled.size[0] != 224) or (scaled.size[1] != 224):\n",
    "        display(orig),display(scaled)\n",
    "        print(scaled.size,orig.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611d579-34a8-4946-975a-738c0c705a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathfxn import get_file_paths\n",
    "\n",
    "# The following three blocks are largely for creating Training and Testing sets in .json files\n",
    "\n",
    "df_dim = pd.read_csv(\"imdb_crop_dimensions.csv\")\n",
    "scaled = get_file_paths(\"./scaled/\")\n",
    "base = []\n",
    "training = []\n",
    "for elem in scaled:\n",
    "    base += [elem[9:]]\n",
    "df_dim['Clean_Path'] = df_dim.apply(lambda x: x[\"Path\"][15:],axis=1)\n",
    "base_df = df_dim[df_dim[\"Clean_Path\"].isin(base)].copy()\n",
    "base_df.apply(lambda x: training.append((x[\"Name\"],x[\"Clean_Path\"])),axis=1)\n",
    "names = base_df[\"Name\"].unique()\n",
    "other_imgs = df_dim[(df_dim[\"Name\"].isin(names))&(~df_dim[\"Clean_Path\"].isin(base))]\n",
    "testing = []\n",
    "for name in names:\n",
    "    person = df_dim[df_dim[\"Name\"]==name]\n",
    "    if len(person) < 10:\n",
    "        sampled = person.sample(len(person))\n",
    "    else:\n",
    "        sampled = person.sample(10)\n",
    "    sampled.apply(lambda x: testing.append((x[\"Name\"],x[\"Path\"])),axis=1)\n",
    "testing[:10],training[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a2da5-7fca-4e09-8606-ae6a18a2cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testing),len(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9867fe-3d9b-4d94-a301-022ef61d05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_test = dict()\n",
    "for i in testing:\n",
    "    json_test[i[1]] = i[0]\n",
    "json.dump(json_test,open(\"Test.json\",\"w\"))\n",
    "json_train = dict()\n",
    "for i in training:\n",
    "    json_train[i[1]] = i[0]\n",
    "json.dump(json_train,open(\"Train.json\",\"w\"))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img_venv",
   "language": "python",
   "name": "img_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
